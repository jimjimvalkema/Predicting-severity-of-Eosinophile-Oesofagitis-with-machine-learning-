---
title: "Eosinofiele Oesofagitis dataset EDA"
author: "Jim Jim Valkema"
date: "9/8/2020"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

## Dataset
TODO neocate en ffed
Nutritional intake was assessed in 40 Dutch adult EoE patients participating in the Supplemental Elemental Trial (SET) using 3-day food diaries. In this randomized controlled trial, diagnosed patients received either a four-food elimination diet alone (FFED) or FFED with addition of an amino acid-based formula (Neocate) for 6 weeks. Disease severity was assessed by peak eosinophil count/high power field (PEC) in esophageal biopsy specimens. Multiple linear regression analyses were performed to assess associations between the intake of nutrients and foods per 1000 kCal and PEC, both at baseline and after the 6 weeks diet, while controlling for baseline variables.
What do vars mean
datatypes, units, values
What is the dependent class variable

## Introduction

## Research question
Can the peak eosinophil baseline in individuals with eosinophilic oesophagitis be predicted with classical machine learning using the measured nutrient intake data?

## Missing data
Missing columns TODO
There are quite a bit of missing attributes and missing values in this data set. Luckily there are plenty of attributes that do have data from where the machine learning algorithm can make its prediction.

37 empty instances
180 empty attributes 
Lots of columns with more then 70 % missing values
```{r echo=FALSE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
library("ggpubr")

# http://www.milanor.net/blog/how-to-open-an-spss-file-into-r/
library(foreign)
#install.packages("janitor")
library(janitor)
#install.packages("memisc")
library("memisc")
library("ggplot2")
#library("unpivotr")
dataset_original = read.spss("../data/Datafile compleet 20-03-2020 voor Hanze SET def.sav", to.data.frame=TRUE, use.value.labels=F, reencode=F, use.missings	=F)
dataset_no_empty <- remove_empty(dataset_original, which = c("rows", "cols"), quiet = FALSE)
#sapply(dataset_no_empty, function(x) sum(is.na(x)))[]

# get amount f missing values
columns.NA <- as.data.frame(colSums(is.na(dataset_no_empty))) # TODO gg plot

# convert to percentage and plot TODO ggplot
get_percentage <- function(x) return((x/nrow(dataset_no_empty))*100) #TODO fix percentages too high
columns.NA <- apply(columns.NA, 1, get_percentage)
names(columns.NA) <- c()
hist(columns.NA, xlab= "percentage of missing values",ylab = "amount of attributes",  main = "Amount of attributes with missing values", breaks = 40)


# remove all attributes with less then 40% na
dataset_clean <- dataset_no_empty[, which(colMeans(!is.na(dataset_no_empty)) > 0.4)]

# ggplot(dataset_clean, aes(value)) +
#   geom_density(adjust = 0.5) +
#   facet_wrap(~variable) + ggtitle( "attributes density")
```
```{r echo=FALSE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
print("columns removed because too many missing values")
ncol(dataset_no_empty)- ncol(dataset_clean)
print("columns romved in total")
ncol(dataset_original)- ncol(dataset_clean)
# TODO do in the markdown text
ncol(dataset_clean)
#colnames(dataset_original)
library(gridExtra)
library(grid)
#grid.table(dataset_clean[1,40])
#grid.table(c(colnames(dataset_original), attributes(dataset_original)))
```

### distrubution
Most attributes seem normally distributed how ever their range differ quite significantly so a normalization step might be required other wise the range can bias the machine learning algorithm to specific attributes.
```{r echo=FALSE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
# TODO plot main attributes from the abstract
library(reshape2)
#TODO check this work around to log 0 values
#dataset_clean <- dataset_clean + 0.00001
# TODO get milk
# TODO where is the cheese lmao
dataset_selected <- dataset_clean[c("PeakEosBaseline_Max","B.Fat.gr", "B.Sat.fat.gr", "B.PUFAS.gr","B.Protein.gr","B.Phosphorus","B.Zinc","B.vitB12", "B.Folate", "B.Calcium","B.Carbohydrates.gr","B.Linoleicacid.gr","Age","Agegroup","Gender","Histological_Response","Neocate")]
#maybe columns: B.Fat.en.pro
# folate is natural occuring in food folate acid is added but they serve the same purpose 
#PUFAS = ploy unsaturated fatty acids

# still 2 empty rows left
dataset_selected <- remove_empty(dataset_selected, which = c("rows", "cols"), quiet = FALSE)
# row 21 has too many missing values in all B. columns
dataset_selected <- dataset_selected[-21,]

# prevent Inf values after logging
dataset_selected[1:12] <- dataset_selected[1:12]+0.000001

nrow(dataset_selected)
logged.dataset <- cbind(log2(dataset_selected[1:12]),dataset_selected[13:17], by="pid")
melted.dataset <- melt(logged.dataset)
nrow(logged.dataset)

ggplot( melt(dataset_selected[c(1:12)]), aes(value)) +
  geom_density(adjust = 0.5) +
  facet_wrap(~variable) + ggtitle( "attributes density") +
  scale_x_continuous(limits = c(0, 300)) + theme_pubr()


ggplot(melt(logged.dataset[c(1:12)]), aes(value)) +
  geom_density(adjust = 0.5) +
  facet_wrap(~variable) + ggtitle( "attributes density logged") +
  scale_x_continuous(limits = c(2, 12))  + theme_pubr()


#cor(logged.dataset)
```
### Normalisation
This is all attributes compared with no normalization, min-max and scaling normalization. 

```{r echo=FALSE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
ggplot(data = melt(logged.dataset[1:12]), aes(x=variable,y=value)) + geom_boxplot(outlier.colour="black", outlier.shape=16,outlier.size=2, notch=F) + ggtitle("all atributes not scaled")

#scale the data
scale_min_max <- function(x){
  x_min = min(x, na.rm = T)
  x_max = max(x, na.rm = T)
  return ((x - x_min) / (x_max - x_min))
}

# TODO choose between age vs agegroup

min_max_norm <- cbind(apply(logged.dataset[1:12], MARGIN = 2, FUN = scale_min_max),logged.dataset[13:17])
scaled_norm <- cbind(scale(logged.dataset[1:12]),logged.dataset[13:17])
scaled_norm_not_logged <- cbind(scale(dataset_selected[1:12]),dataset_selected[13:17])
# TODO better titles

ggplot(data = melt(min_max_norm[1:12]), aes(x=variable,y=value)) + geom_boxplot(outlier.colour="black", outlier.shape=16,outlier.size=2, notch=F) + ggtitle("all atributes min max normalisation")+ theme_pubr()
ggplot(data = melt(scaled_norm[1:12]), aes(x=variable,y=value)) + geom_boxplot(outlier.colour="black", outlier.shape=16,outlier.size=2, notch=F) + ggtitle("all atributes scaling normalisation")+ theme_pubr()

ggplot(melt(scaled_norm[c(1:12)]), aes(value)) +
  geom_density(adjust = 0.5) +
  facet_wrap(~variable) + ggtitle( "attributes density normalised and logged") + theme_pubr()

ggplot(melt(scaled_norm_not_logged[c(1:12)]), aes(value)) +
  geom_density(adjust = 0.5) +
  facet_wrap(~variable) + ggtitle( "attributes density normalised not logged")  + theme_pubr()
```

```{r echo=FALSE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
library("ggcorrplot")
#TODO
#test <- remove_empty(scaled_norm, which = c("rows", "cols"), quiet = FALSE)
#
# TODO better attribute names
# TODO exclude neocate
# split the data in 2 because it wont fit in 1 plot
scaled_norm.1 <- scaled_norm[,1:8]
# add the first columns because that is PeakEos which is our attribute we want to predict
scaled_norm.2 <- scaled_norm[,c(1,9:17)]
cor.data.scaled.1 <- cor(scaled_norm.1)
cor.data.scaled.2 <- cor(scaled_norm.2)
ggcorrplot(cor.data.scaled.1,  ggtheme = ggplot2::theme_gray,hc.order = T,lab = T, digits=1) + ggtitle( "correlation matrix of the bones lenght and width") #+ theme_pubr()
ggcorrplot(cor.data.scaled.2,  ggtheme = ggplot2::theme_gray,hc.order = T,lab = T, digits=1) + ggtitle( "correlation matrix of the bones lenght and width") #+ theme_pubr()
#TODO check if histological response
```
### splitting Peak eos baseline max into catagories
The Peak eos baseline max needs to be split into catagories because most classical machine learning algorithms cant use a numerical dependent class variable. The Peak eos baseline max is split into three catagories: low, mid, high split at 0-49, 49-75, 75-200. This seems to create a a most even distrubution of instances while also being informative. It could be split into more catagories but this would make it harder to create a machine learining model with high accuracy.

TODO weights


```{r echo=FALSE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
factor_columns <- c("Gender","Histological_Response","Neocate")
#scaled_norm[,factor_columns] <- as.factor(as.character(scaled_norm[,factor_columns]))

peak.eos.max <- remove_empty(dataset_clean[23], which = c("rows", "cols"), quiet = FALSE)

ggplot(melt(peak.eos.max), aes(value)) +
  geom_histogram(adjust = 0.5, bins = 40) +
  facet_wrap(~variable) + ggtitle( "attributes density normalised not logged") + theme_pubr()

scaled_norm$PeakEosBaseline_Max <- cut(as.matrix(peak.eos.max), breaks = c(0,49,75,200), labels = c("low","mid","high"))[-21]
# TODO check if the gender is correct
scaled_norm$Gender <- cut(as.matrix(scaled_norm$Gender), breaks = c(-1,0,1), labels = c("male","female"))
scaled_norm$Histological_Response <- cut(as.matrix(scaled_norm$Histological_Response), breaks = c(-1,0,1,2), labels = c("1st","2nd","3rd"))
scaled_norm$Neocate <- cut(as.matrix(scaled_norm$Neocate), breaks = c(-1,0,1), labels = c("no response","response"))
write.csv(scaled_norm,"../data/selected_data_logged_scaled.csv", row.names = FALSE)

summary(scaled_norm$PeakEosBaseline_Max)
```
## quality metrics
The most important metric to optimise for is the sensistivity a false positive case is less harmfull then a false negative case. This is because a patient that is clasified as sick but turns out to be healthy after further inspection, experiences far less harm then those who are clasified as healthy but continue suffering from symptoms.  

The algorithm should be able to run on a consumer desktop and but doesn't need to be too fast when classefying. This because it is likely that only one patient will be avaluated at the time so accuracy is far more important then speed.


## model evaluation
j48 all: 28.2051% acc

j48 cfs subset eval: 38.4615
weka.attributeSelection.ClassifierSubsetEval -B Evaluator:    weka.attributeSelection.ClassifierSubsetEval -B weka.classifiers.trees.J48 -T -H "Click to set hold out or test instances" -E DEFAULT -- -C 0.25 -M 2
Search:       weka.attributeSelection.GreedyStepwise -T -1.7976931348623157E308 -N -1 -num-slots 1

j48 ClassifierAttributeEval + ranker:
top 7: 35.8974
top 6: 30.7692
top 5: 33.3333
top 4: 33.3333

Evaluator:    weka.attributeSelection.ClassifierAttributeEval -execution-slots 1 -B weka.classifiers.trees.J48 -F 5 -T 0.01 -R 1 -E DEFAULT -- -C 0.25 -M 2
Search:       weka.attributeSelection.Ranker -T -1.7976931348623157E308 -N -1

j48 ClassifierAttributeEval + ranker leave one out:
top 7: 33.3333
top 6: 33.3333 
top 5: 35.8974
top 4: 43.5897

Evaluator:    weka.attributeSelection.ClassifierAttributeEval -L -execution-slots 1 -B weka.classifiers.trees.J48 -F 5 -T 0.01 -R 1 -E DEFAULT -- -C 0.25 -M 2
Search:       weka.attributeSelection.Ranker -T -1.7976931348623157E308 -N -1


```{r}
weka.results <- read.csv("../data/results/results_attribute_selection_selected_data_logged_scaled.csv")

confusion.df.all <- data.frame()

for (dataset.df in split.data.frame(weka.results,weka.results$Key_Dataset)) {
  for (algo.df in split.data.frame(dataset.df,dataset.df$Key_Scheme)) {
    for (options.df in split.data.frame(algo.df,algo.df$Key_Scheme_options)) {
      new.row <- data.frame(
            algo = options.df$Key_Scheme[1]
            ,key.dataset = options.df$Key_Dataset[1]
            ,acc = mean(options.df$Percent_correct)
            ,speed_test = mean(options.df$UserCPU_Time_testing)
            ,speed_train = mean(options.df$UserCPU_Time_training)
            ,TP = median(options.df$Num_true_positives)
            ,TN = median(options.df$Num_true_negatives)
            ,FP = median(options.df$Num_false_positives)
            ,FN = median(options.df$Num_false_negatives)
            ,key.scheme_options = options.df$Key_Scheme_options[1]
            )
      
          confusion.df.all <- rbind(confusion.df.all,new.row)
    }
  }
}

confusion.df.all

for (dataset.df in split.data.frame(confusion.df.all,confusion.df.all$key.dataset)) {
  print(dataset.df)
}
```

```{r}
dataset_clean <- dataset_clean[-c(21,39,40), ]
dataset_clean$PeakEosBaseline_Max <- cut(as.matrix(peak.eos.max), breaks = c(0,49,75,200), labels = c("low","mid","high"))[-21]
#logged.dataset.full <- cbind(log2(dataset_clean[1:12]),dataset_clean[13:17], by="pid")
dataset_clean
```

```{r}
write.csv(dataset_clean,"../data/cleaned-data-all-no-scaling-and-norm.csv", row.names = FALSE)

# get averages 
j48_cfs_eval_attribute <- c("B.PUFAS.gr","B.Calcium","B.Carbohydrates.gr","B.Linoleicacid","gr,Gender","Neocate","PeakEosBaseline_Max")
dataset_clean[c("B.Linoleicacid","gr,Gender","Neocate","PeakEosBaseline_Max")]
```