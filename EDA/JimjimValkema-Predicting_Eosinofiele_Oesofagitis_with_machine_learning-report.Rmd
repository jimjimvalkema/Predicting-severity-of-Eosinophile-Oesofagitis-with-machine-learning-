---
title: "Predicting Eosinofiele Oesofagitis with machine learning - report"
author: "Jim Jim Valkema"
date: "11/16/2020"
output:
  html_document:
    df_print: paged
---
---
title: "Birds' bones and living habits"
author: "Jimjim Valkema"
date: "9/10/2019"
output:
  pdf_document: default
  html_document: default
---
## data set  
TODO codebook

## introduction  
Eosinphilic esophagitis is a food allergy in the esophagus. The allergy causes a type of white bloodcells called eosinphilis to build up in the lining of the esophagus which cause swelling, pain and discomfort. The allergy gets triggered by different types of food for different people. These triggers are usually found by making the patient follow a elimination diet.   
Eosinphilic esophagitis is diagnosed by a gastroenterologistc who will examen the esophagus with a endoscope while the patient is under anesthesia and take a biopsy from the esophagus if needed. This is a moderatly invasive procedure for diagnosis and that is why there is a another method of diagnoses researched in this study. This study proposes a method of diagnosis by measuring certain blood markers and identifying patients whith eosinphilic esophagitis with a machine learning algorithm.    
  

## materials and methods

### data  
Nutritional intake was assessed in 40 Dutch adult EoE patients participating in the Supplemental Elemental Trial (SET) using 3-day food diaries. In this randomized controlled trial, diagnosed patients received either a four-food elimination diet alone (FFED) or FFED with addition of an amino acid-based formula (Neocate) for 6 weeks. Disease severity was assessed by peak eosinophil count/high power field (PEC) in esophageal biopsy specimens. Multiple linear regression analyses were performed to assess associations between the intake of nutrients and foods per 1000 kCal and PEC, both at baseline and after the 6 weeks diet, while controlling for baseline variables.

### software  
TODO versions and double check missing libraries
The exploratory data analysis has been done using rmarkdown with the following packages: 
* ggplot: for creating plots    
* gridExtra: further styling plots  
* reshape2: transform data from wide and long formats  
* rlist: appending items in a list  
* ggcorrplot: creating a correlation plot  
  
  
Creating the machine learning model has been done in weka.  
  
Development of the wekawrapper has been done in intellij with the java programming language.  
  
### exploratory data analysis   
TODO show boxplot and density, splitting peak eos

There are quite a bit of missing attributes and missing values in this data set. Luckily there are plenty of attributes that do have data from where the machine learning algorithm can make its prediction. Figure 1 shows that a lot of attributes miss more than 70% of their values which in combination with the few instances of this data set means that those attribute are not of value to a machine learning algorithm and are there for removed in this study. However the data does only has 42 instance so training a algorithm could result in overfitting.     

The data has also been log two transformed so that it would better fit a normal distrubution (figure 2 TODO density plot) which act more predictable with most machine learning algorithms. Further more the data has been normalised (Figure 3 TODO boxplot) with scaling normalisation. Scaling normalisation normalises the data with the standard deviation which has the advantage of being able to scale fututere measurements even when the might range outside of the training data set unlike min-max normalisation. This is espacialy important for this data set since it contains such few instances. 

It seems that the majority isn't highly correlated which meant that there is enough information between attributes for the algorithm to train on. Some attributes like the fats (B.Sat.fat.gr, B.Fat.gr, etc) are highly correlated.  So only one of these ended up in the final training data set because the others convey very little information to algorithm. In this case it ended up being the poly unsaturated fatty acids (B.PUFAS.gr) who ended up in the final training data set during the attribute selection.  

### evaluation metrics   
The most important metric to optimise for is the sensitivity a false positive case is less harmful then a false negative case. This is because a patient that is classified as sick but turns out to be healthy after further inspection, experiences far less harm then those who are classified as healthy but continue suffering from symptoms.   
The speed at which algorithm runs at isn't specificly optimised for nor is it measured on larger data sets. However it's main usecase would be to classiefy individual patients which it can do near instantly in a common desktop envirement.

   
### Exploring algorithms  
Model evaluation is done in the weka experimenter with five data sets and seven algorithms (default setting from weka 3.8.4) and four variations of the stacking meta algorithm with different settings. The best performing algorithm is believed to be the meta stacking algorithm with the correlation-based feature selection dataset. This algorithm has a accuracy of 47%. The meta stacking algorithm consist of random forest as its meta learner and the following algorithms are its ensamble members: J48, Random Forest, naive Bayes and logistic regression. Some algorithms like oneR and k nearest neighbor aren't used since they do not create a true model and there for arn't usefull in this study.    

### Attribute selection  
There are different data sets created from the original dataset which is logged and normalised. The different datasets are created by using different attribute selection methods.   
These methods are: 
classifier attribute evaluation with j48. Done with the setting leave one out turned on and with it off. And with differnt rank cuttof points.     
And correlation-based feature selection.
The data set created from the Correlation-based feature selection yielded the best results and is used in the final model.  
The attributes selected in this data set are: Neocate, B.Carbohydrates.gr, Gender, B.PUFAS.gr, B.Linoleicacid.gr, B.Calcium


### links:
Java wrapper: https://github.com/jimjimvalkema/Predicting-severity-of-Eosinophile-Oesofagitis-with-machine-learning-/tree/master/app  
Rmarkdown EDA log: https://github.com/jimjimvalkema/Predicting-severity-of-Eosinophile-Oesofagitis-with-machine-learning-/blob/master/EDA/Eosinofiele-Oesofagitis-dataset-EDA.Rmd  

## results    
The resulting best performing algorithm is a meta stacking algorithm with random forest as its meta-learner and the following ensamble members: J48, Random Forest, naive Bayes and logistic regression.   
This model is trained on a dataset that is created from correlation-based feature selection on the original data that is log two transformed, normalised with scaling normalisation and without the attributes that have more than 40% of their values missing.  
This model can be used in a command line interface to classify a patient in three catogories: low,mid,high based on six attributes with 47% accuracy.
This commandline interface can process the raw data and can do the neccesary pre-processing like normalisation and log transform. It can process csv, arf files and handle a single instance provided in the command line.  
 
## discussion   
what if new insight with this analysis of ml algo? EOS might be complex issue? param selection? No wieght on false negatives?
While the accuracy of the model created by this study isn't that high, it might improve when more data becomes available. There are also a lot more attributes in this data set that might yield a better model and give further insights. Finding the more valuable attribute requires further automation in processing and attribute selection.    
It might also be interesting to see if model can be created to predict the food that triggered the allergy which could help speed up illimination diets. However it is believed that this requires a different data set with more specific information about a patients diet. 

There only a few of the many attributes that are looked at in this study. These attributes were picked based
on the findings of the previous study. This is because of time limitations however it could be interesting
to look at the rest of the data with more time efficient techniques and tools.   
There are also a fair amount of missing values removed which might still be valuable because a missing value on its own can also convey
information in some cases.  


## Conclusion 
The resulting model is believed to be not accurate enough to be used in a clinical setting. It also found that the data set is too small to evaluated if a proper model can be created.   
It is found that scaling by the standard deviation would be the most effective method for normalization for
a future proof machine learning model.
The data also benefits from a log2 transformation because it seems to be more normal distributed this way.   
It is also found that some attribute like the different types of fat are correlated and that most attribute selection methods tend to prefer only one of the sub types of fat since each individual fat type do not convey a lot of information to those attivube selection algorithms.

## project proposal  
Further improvement of the accuracy of the model could be explored in the high throughput biocomputing
minor. A more advanced algorithm like a neural network could be trained to possibly get a higher accuracy.
Getting a larger dataset would also be beneficial to prevent overfitting. A automated method of selecting and-pre processing attributes can then also be developed.  

Developing a simple user interface might also be a great improvement if this algorithm is used in a clinical setting.
This user Interface could provide a way for the user to provide the data and perhaps a simple way to edit
it. Having a graphical representation accompanied with general statistics of the resulting classifications
could help users that do not have a deep understanding of machine learning to interpret the output of the
algorithm.
Developing an application for desktop computing on platforms like linux,mac and windows would be most
useful since mobile applications would be more work to develop while not gaining much usability given the
context. A web app could be useful however the app benefits from being verifiably open source
when running locally. This is because researchers need to know exactly what program they are running in
order for their work to be reproducible.
    
## references  
data source: 
Eosinphilic Esophagitis: http://www.pghclinic.com/wp-content/uploads/2019/05/56.pdf


