---
title: "JimjimValkema-EDA-conclusie-discussie"
output: pdf_document
---

---
title: "Eosinofiele Oesofagitis dataset EDA"
author: "Jim Jim Valkema"
date: "9/8/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---
## conclusion
This data set has a great amount of attributes however it does suffer from a great amount of missing values and only has 39 usable instances (fig 1). The data has 155 attributes that have more than 60% missing data excluding completely empty attributes. However there are 544 attributes left when the empty attributes and those with more than 60% missing are removed. But it might be challenging to effectively train a machine learning algorithm on a data set with only 39 instances.       
The ranges of different attributes are quite significant (fig 2) so the data needs to be normalized.
It is found that scaling by the standard deviation would be the most effective method for normalization (fig 3) for a future proof machine learning model. This is because this type of normalization can handle data that falls outside of the range of the training data set. Unlike min max normalization which fails in this regard. 
It is important that the end product can handle these ranges because the training data set has only a few instances and therefore might not contain the entire range of values for each attribute.
The data also benefits from a log2 transformation because it seems to be more normal distributed this way (fig 4 & 5).
Most attributes that were mentioned by the previous study are not correlated with each other (fig 6 & 7). Which could be an indication that they contain a good amount of information for the machine algorithm to train on.

## discussion
There only a few of the many attributes that are looked at in this study. These attributes were picked based on the findings of the previous study. This is because of time limitations however it could be interesting to look at the rest of the data with more time efficient techniques and tools. However the final product can't have too many attributes as its input because that would be too cumbersome to use and most machine learning algorithms would probably benefit much from this many attributes. 
There are also a fair amount of missing values removed which might still be valuable because a missing value on its own can also convey information in some cases.

```{r include=FALSE, echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, warning=FALSE}
# http://www.milanor.net/blog/how-to-open-an-spss-file-into-r/
library(foreign)
#install.packages("janitor")
library(janitor)
#install.packages("memisc")
library("memisc")
library("ggplot2")
#library("unpivotr")
dataset_original = read.spss("/home/jimjim/Desktop/Predicting-severity-of-Eosinophile-Oesofagitis-with-machine-learning-/data/Datafile compleet 20-03-2020 voor Hanze SET def.sav", to.data.frame=TRUE, use.value.labels=F, reencode=F, use.missings	=F)
dataset_no_empty <- remove_empty(dataset_original, which = c("rows", "cols"), quiet = T)
#sapply(dataset_no_empty, function(x) sum(is.na(x)))[]

# get amount f missing values
columns.NA <- as.data.frame(colSums(is.na(dataset_no_empty))) # TODO gg plot

# convert to percentage and plot TODO ggplot
get_percentage <- function(x) return((x/nrow(dataset_no_empty))*100) #TODO fix percentages too high
columns.NA <- apply(columns.NA, 1, get_percentage)
names(columns.NA) <- c()
```
```{r include=T, echo=FALSE, cache=FALSE, results=FALSE, warning=FALSE, comment=FALSE, warning=FALSE}
hist(columns.NA, xlab= "percentage of missing values",ylab = "amount of attributes",  main = "Amount of attributes with missing values", breaks = 40)
# remove all attributes with less then data
dataset_clean <- dataset_no_empty[, which(colMeans(!is.na(dataset_no_empty)) > 0.4)]

# ggplot(dataset_clean, aes(value)) +
#   geom_density(adjust = 0.5) +
#   facet_wrap(~variable) + ggtitle( "attributes density")
```
*Fig 1: The amount of attributes on the y axis and the percentage of missing values these attributes have. This figure shows that a lot of attributes has below 20% missing but most notably are those with around 60% missing.*

```{r include=FALSE, echo=FALSE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
print("columns removed because too many missing values")
ncol(dataset_no_empty)- ncol(dataset_clean)
print("columns removed in total")
ncol(dataset_original)- ncol(dataset_clean)
print("columns total")
ncol(dataset_clean)
# TODO do in the markdown text
#ncol(dataset_clean)
#colnames(dataset_original)
library(gridExtra)
library(grid)
#grid.table(dataset_clean[1,40])
#grid.table(c(colnames(dataset_original), attributes(dataset_original)))
```
```{r include=FALSE, echo=FALSE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
# TODO plot main attributes from the abstract
library(reshape2)
#TODO check this work around to log 0 values
dataset_clean <- dataset_clean + 0.00001
# TODO get milk
# TODO where is the cheese lmao
dataset_selected <- dataset_clean[c("Neocate", "PeakEosBaseline_Max" ,"Gender","Age","Agegroup","Histological_Response","B.Fat.gr", "B.Sat.fat.gr", "B.PUFAS.gr","B.Protein.gr","B.Phosphorus","B.Zinc","B.vitB12", "B.Folate", "B.Calcium","B.Carbohydrates.gr","B.Linoleicacid.gr")]
#maybe columns: B.Fat.en.pro
# folate is natural occuring in food folate acid is added but they serve the same purpose 
#PUFAS = ploy unsaturated fatty acids

dataset_selected_numeric <- dataset_clean[c("PeakEosBaseline_Max","Age","Agegroup","Histological_Response","B.Fat.gr", "B.Sat.fat.gr", "B.PUFAS.gr","B.Protein.gr","B.Phosphorus","B.Zinc","B.vitB12", "B.Folate", "B.Calcium","B.Carbohydrates.gr","B.Linoleicacid.gr")]

# still 2 empty rows left
dataset_selected_numeric <- remove_empty(dataset_selected_numeric, which = c("rows", "cols"), quiet = T)
# row 21 has too many missing values in all B. columns
dataset_selected_numeric <- dataset_selected_numeric[-21,]

# prevent Inf values after logging
dataset_selected_numeric <- dataset_selected_numeric+0.000001

logged.dataset <- log2(dataset_selected_numeric)
melted.dataset <- melt(logged.dataset)
```

```{r echo=FALSE, cache=FALSE, warning=FALSE, comment=FALSE, warning=FALSE,results='hide'}
ggplot(data = melt(logged.dataset), aes(x=variable,y=value)) + geom_boxplot(outlier.colour="black", outlier.shape=16,outlier.size=2, notch=F) + ggtitle("all atributes not scaled")

```
*Figure 2: Shows the selected attributes distributions. Note that the range differ significantly which can interfere with most machine learning algorithms.*

```{r echo=FALSE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
#scale the data
scale_min_max <- function(x){
  x_min = min(x, na.rm = T)
  x_max = max(x, na.rm = T)
  return ((x - x_min) / (x_max - x_min))
}

# TODO why do those attributes disappear also bepa gone?
min_max_norm <- as.data.frame(apply(logged.dataset, MARGIN = 2, FUN = scale_min_max))
scaled_norm <- as.data.frame(scale(logged.dataset))
# TODO better names

#ggplot(data = melt(min_max_norm), aes(x=variable,y=value)) + geom_boxplot(outlier.colour="black", outlier.shape=16,outlier.size=2, notch=F) + ggtitle("all atributes min max normalisation")
ggplot(data = melt(scaled_norm), aes(x=variable,y=value)) + geom_boxplot(outlier.colour="black", outlier.shape=16,outlier.size=2, notch=F) + ggtitle("all atributes scaling normalisation")

```
*Figure 3: Shows the distributions of the same attributes as figure 2 but with scaling normalisation which makes for much more manageable ranges.*

```{r echo=FALSE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
ggplot( melt(dataset_selected_numeric), aes(value)) +
  geom_density(adjust = 0.5) +
  facet_wrap(~variable) + ggtitle( "attributes density")
```
*Figure 4: show the distribution of the attributes on a density graph. A lot of data is completely 
shifted toward the x axes which show that it might benefit from a log2 transform to achieve a more normal distribution.*


```{r echo=FALSE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
ggplot(melt(logged.dataset), aes(value)) +
  geom_density(adjust = 0.5) +
  facet_wrap(~variable) + ggtitle( "attributes density logged")


#cor(logged.dataset)
```
*Figure 5: Show the data after a log2 transform which makes for a dataset that appears to be normal distributed.*
```{r echo=FALSE, cache=FALSE, results=TRUE, warning=FALSE, comment=FALSE, warning=FALSE}
library("ggcorrplot")
#TODO
#test <- remove_empty(scaled_norm, which = c("rows", "cols"), quiet = FALSE)
#
# TODO better attribute names
# TODO exclude neocate
# split the data in 2 because it wont fit in 1 plot
scaled_norm.1 <- scaled_norm[,1:8]
# add the first columns because that is PeakEos which is our attribute we want to predict
scaled_norm.2 <- scaled_norm[,c(1,9:15)]
cor.data.scaled.1 <- cor(scaled_norm.1)
cor.data.scaled.2 <- cor(scaled_norm.2)
ggcorrplot(cor.data.scaled.1,  ggtheme = ggplot2::theme_gray,hc.order = T,lab = T, digits=1) + ggtitle( "correlation matrix of the bones lenght and width")
ggcorrplot(cor.data.scaled.2,  ggtheme = ggplot2::theme_gray,hc.order = T,lab = T, digits=1) + ggtitle( "correlation matrix of the bones lenght and width")
```
*Figure 6 & 7: show the correlation between attributes. It seems that the majority isn't highly correlated which might be a good indication that there is enough information between attributes for the algorithm to train on.*